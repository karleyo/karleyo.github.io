<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>123</title>
      <link href="/2022/12/21/123/"/>
      <url>/2022/12/21/123/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>第一篇文章</title>
      <link href="/2022/12/19/2022-12-19/"/>
      <url>/2022/12/19/2022-12-19/</url>
      
        <content type="html"><![CDATA[<h2 id="这是我的第一篇文章"><a href="#这是我的第一篇文章" class="headerlink" title="这是我的第一篇文章"></a>这是我的第一篇文章</h2><p><strong>Neural Machine Translation by Jointly Learning to Align and Translate</strong></p><pre><code>这篇论文由Dzmitry Bahdanau、 KyungHyun Cho 和 Yoshua Bengio 于2015年在深度学习领域的顶级会议ICLP会议上发表。作者提出了一种通过允许模型自动搜索源句子中与预测目标词相关的部分，而不必将这些部分显示分割的解码方法。作者通过这种新的方法达到了与现有的最先进的基于短语的英法翻译系统相当的翻译效果，并通过定性实验发现模型的对齐方式与作者的intuition一致。</code></pre><blockquote><pre><code>This paper was presented in 2015 by Dzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio at the ICLP conference, a top conference in deep learning. The authors propose a decoding method by allowing the model to automatically search for parts of the source sentence that are relevant to the predicted target word, without having to segment these parts of the display. With this new approach the authors achieve translation results comparable to existing state-of-the-art phrase-based English-French translation systems, and find through qualitative experiments that the alignment of the model is consistent with the authors&#39; intuition.</code></pre></blockquote><pre><code>作者在引言部分首先对神经网络机器翻译以及encoder-decoder的工作机制进行了阐述。之后引出了encoder-decoder方法存在的潜在问题，神经网络需要将源句中所有的必要信息压缩成定长的向量，这可能使神经网络难以处理长句子，尤其是那些比训练语料库中更长的句子。作者为了解决这一问题引入了一种对encoder-decoder模型的拓展，每当生成的模型在翻译中生成一个单词的时候，它会搜索源句中最相关信息集中的位置。然后，该模型根据与源句位置相关的上下文向量和之前产生的所有目标词来预测目标词。同时作者阐述了改进的模型与原来的区别以及新模型翻译性能的提升。</code></pre><blockquote><pre><code>The authors begin the introduction with an explanation of neural network machine translation and the mechanism by which encoder-decoder works. A potential problem with the encoder-decoder approach is then introduced, where the neural network needs to compress all the necessary information in the source sentence into a fixed-length vector, which may make it difficult for the neural network to handle long sentences, especially those that are longer than those in the training corpus. The authors introduce an extension to the encoder-decoder model to address this problem, whereby whenever the generated model generates a word in a translation, it searches for the location in the source sentence where the most relevant information is concentrated. The model then predicts the target word based on the context vector associated with the position of the source sentence and all previously generated target words. The authors also describe the differences between the improved model and the original and the improvement in translation performance of the new model.</code></pre></blockquote><pre><code>之后作者对神经机器翻译用到的神经网络框架进行了回顾，并从数学角度对RNN Encoder-Decoder进行了阐述。然后介绍了本文提出的一种新的框架，新的框架由一个双向的RNN作为编码器和一个在解码翻译时对源句模拟搜索的解码器组成。</code></pre><blockquote><pre><code>The authors then review the neural network frameworks used for neural machine translation and describe the RNN Encoder-Decoder from a mathematical perspective. A new framework proposed in this paper is then presented, which consists of a bidirectional RNN as an encoder and a decoder that simulates search on the source sentence when decoding the translation.</code></pre></blockquote><pre><code>作者在实验部分比较了原始的RNN Encoder-Decoder模型和本文提出的改进模型，让这两个模型分别在句子最大长度为 30 和 50 的训练集上训练，并进行了定量分析和定性分析。通过实验作者进一步证明了提出的模型相较于原始的Encoder-Decoder模型在处理长句子方面的优势。</code></pre><blockquote><pre><code>In the experimental section, the authors compare the original RNN Encoder-Decoder model with the improved model proposed in this paper and have the two models trained on training sets with maximum sentence lengths of 30 and 50, respectively, and perform both quantitative and qualitative analyses. Through experiments the authors further demonstrate the advantages of the proposed model over the original Encoder-Decoder model in handling long sentences.</code></pre></blockquote><pre><code>最后作者对本文提出的一种通过允许模型自动搜索源句子中与预测目标词相关的部分，而不必将这些部分显示分割的解码方法进行了总结，并且通过实验结果表明，无论句子长度如何，本文所提出的RNN Search都显著优于传统的Encoder-Decoder模型，且对源句子长度有更强的鲁棒性。并得出了该模型能够在产生正确翻译的同时，正确地将每个目标词与源句中的相关词或它们的注释对齐的结论。与此同时，作者还提出了一个神经机器翻译未来会面临的挑战。</code></pre><blockquote><pre><code>Finally the authors summarise a decoding method proposed in this paper by allowing the model to automatically search for parts of the source sentence that are relevant to the predicted target word without having to segment these parts of the display, and show through experimental results that the RNN Search proposed in this paper significantly outperforms the traditional Encoder-Decoder model regardless of the sentence length and is The proposed RNN Search significantly outperforms the traditional Encoder-Decoder model regardless of the sentence length and is more robust to the source sentence length. It is also concluded that the model is able to correctly align each target word with the relevant words in the source sentence or their annotations while producing the correct translation. At the same time, the authors present a challenge that neural machine translation will face in the future.</code></pre></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/12/18/hello-world/"/>
      <url>/2022/12/18/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
